# Relation Extraction

## Transformer-based Relation Extraction
The core idea of my approach is to use the contextual embeddings of the two entities generated by a transformer-based model to decide whether there is a relation between them.

This idea is similar to the paper: https://arxiv.org/abs/1906.03158. More specifically, compared with the original transformer-based model, e.g., BERT, ALBERT, the following changes are made:

* Input: In addition to the standard input, entity markers---[E1], [/E1], [E2], [/E2]---are added so that the model knows the positions of the entities of interest. 
* Output: Rather than using the output corresponding to the [CLS] token as the sentence representation, the embeddings for [E1] and [E2] are concatenated to represent the relation instead.
* Classification: a single layer feed forward network is added as the classification layer.

## Code Explanation
Since the above mentioned paper has been implemented here: https://github.com/plkmo/BERT-Relation-Extraction, my implementation is based on this repo. However, and I've made a number of changes:

* In terms of the overall structure,
    * The code for preprocessing is separated from the training code;
    * $k$-fold cross validation is added for training;
    * Separate test code is added.
* Replace the two entities, "GENE" and "DISEASE", with words sampled randomly from the vocabulary of the tokenizer, since all the sentences have the same two entities. 

### Pipeline and Code Structure
The pipeline is as follows:
1. Preprocessing
    1. Load pre-trained BERT/ALBERT model, e.g., bert-base-uncased, albert-base-v2, and the corresponding tokenizer;
    1. Load and tokenize the training and test data.
1. $k$-fold cross validation
1. Estimate the trained model on the test data.
1. Use the trained model for inference.

Code structure:
- /BERT_Relation_Extraction
  - /src
    - /model
      - /ALBERT # ALBERT model, including model, tokenization and configuration
      - /BERT # BERT model, including model, tokenization and configuration
    - /cross_val.py # $k$-fold cross validation: split the data into training and validation 
    - /infer.py # use a trained model to detect relation in a sentence entered by the user
    - /misc.py
    - /preprocessing_funcs.py # load the pretrained model and tokenizer, load training and test data, define dataset
    - /test.py # script to run model on test set.
    - /train.py # script to train the model
    - /train_funcs.py # Load model state and results, evaluate
  - /pipeline.py # a pipeline to use your trained model to detect relations(RE). 
  - readme.md

## Requirements
botocore==1.21.36
tqdm==4.51.0
pandas==1.1.4
six==1.12.0
numpy==1.18.2
tensorflow==1.14.0
requests==2.21.0
transformers==4.0.0
boto3==1.18.36
sentencepiece==0.1.96
torch==1.9.0+cpu
filelock==3.0.12
scikit_learn==0.24.2

## Running the code
python3 pipeline.py